{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : Preserving conversational history and generating summary at the end of converstaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install  langgraph\n",
    "! pip install langchain \n",
    "! pip install langchain_core_google_genai \n",
    "! pip install typing_extensions \n",
    "! pip install langchain_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, add_messages , MessagesState\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict \n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage ,RemoveMessage,SystemMessage\n",
    "from _collections_abc import Sequence\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    summary:str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-flash-latest\", temperature=0, seed  =365)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING ask_question:\")\n",
    "    \n",
    "    question = \"What is your question?\"\n",
    "    print(question)\n",
    "    \n",
    "    return State(messages = [AIMessage(question), HumanMessage(input())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING chatbot:\")\n",
    "    for i in state[\"messages\"]:\n",
    "        i.pretty_print()\n",
    "    \n",
    "    system_message=f\"here's quick summary of what's been disscuss so far: {state.get('summary', '')} . keep this in mind as you answer the next question.\"\n",
    "    response = chat.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(state: State) -> State:\n",
    "    print(f\"\\n-------> ENTERING trim_messages:\")\n",
    "    \n",
    "    new_conversation = \"\"\n",
    "    for i in state[\"messages\"]:\n",
    "        new_conversation += f\"{i.type}: {i.content}\\n\\n\"\n",
    "        \n",
    "    summary_instructions = f'''\n",
    "Update the ongoing summary by incorporating the new lines of conversation below.  \n",
    "Build upon the previous summary rather than repeating it so that the result  \n",
    "reflects the most recent context and developments.\n",
    "Previous Summary:\n",
    "{state.get(\"summary\", \"\")}\n",
    "\n",
    "New Conversation:\n",
    "{new_conversation}\n",
    "'''\n",
    "    \n",
    "    print(summary_instructions)\n",
    "    \n",
    "    summary = chat.invoke([HumanMessage(summary_instructions)])\n",
    "    \n",
    "    remove_messages = [RemoveMessage(id = i.id) for i in state[\"messages\"][:]]\n",
    "    \n",
    "    return {\"messages\":remove_messages, \"summary\":summary.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_node(\"summarize_messages\", summarize_messages)\n",
    "\n",
    "graph.add_edge(START, \"ask_question\")\n",
    "graph.add_edge(\"ask_question\", \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", \"summarize_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkerpoint= InMemorySaver()\n",
    "graph_compiled = graph.compile(checkerpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Test the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------> ENTERING ask_question:\n",
      "What is your question?\n",
      "\n",
      "-------> ENTERING chatbot:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "What is your question?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was he known for/\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'type': 'text', 'text': 'I apologize, but the summary you mentioned was empty, so I don\\'t know who \"he\" is.\\n\\nCould you please provide the name of the person we were discussing so I can tell you what he was known for?', 'extras': {'signature': 'CoMHAXLI2nzezAsLrSj/7XRgQIG6FYHuek3tYWtgjmLzfht8GMIsolSlWcUjPh3fYTnU0mE/XP4vBsRy0Ka+wPpzid8vukbAmn7XdiJE9qw19pA0EqiI3bHS6SW4gD1WY9Y+q/XozGXBZOExU37pJ1IGROJUCDU0Hdc56pGW7n4q9E7S32X9RjLks6Aip3ct3nX3YPJPJaYqOVdzm8Wr7E0l4Jc4WarzE3ymX/uIMRoXHl0fUwEOztj3vLglbqc5XnPMNsqX4ORSXmyTUWo5vh/oDF37Ik7MA1eGnIIL7Ku7HcE+mEU0X05mMS8Ffp1hT3L4LozddDGs7zz3ql6maZjOZvbdvn5d0+0fM0x4qjyNIF6S4Apr+nNqsgeiPRveTC//7+kbMkz9j0YK9GR7iqe/6WpXpfpG+TH9m5+etEjgmo5tGUETB4BGFAv0ybkRvTK7+JNVLlZLMa7gTfL0KIlcdRvNfJHUp9oBCD/kZZJ8Re4e/GSk2BsQyytf5e5j+Em+CukTlygcQy5pvsPKR/GVfY9zcQI6BE3DlIoE2NWtQzP+JHgC2BXiDv9YInYpzAdPdfQ3Cr5+4KcNy5KWWiumdftslwYy7ZqQFpU0+3PaneGEi5qKvmqBdOLZc4xgXy+35UmadDp6VFMkuVbydKgb7rXU+UdMJ3oVtXxFUluUawvfYLJ0hE7XETsq1owOmD3bvO9l6jIHprrNuABByGHhpx94sGoMHlGL8DSjJCg5qNLp42/uLIReR0ooqM7IyEv1k/7VoJ/iA6eLZchKl4qZaIvWC16m4BYs2oS6zUuV8FaSUfbABK/Cdb641sqzhT4lJpJ2VETslhNG4EtV8nViosgruVNCTYRnx1kCVWsgArukZbny0ZQC37JaPY1qlubenDjwcQl0jNjGmoVpgSgt3aBdPeySCSCo3xD5dXMm0YuLA2oZ4HIdhxY6sjKd+OSmpQQkNR/sByD8mysWBUuSIPlLzWCJQxZVCZTN6ezkTfXDsRoRUDKYNwujAGG1OAQCBNhdVojJr8wEpAE76M4SbRrQfLmfGC+q625ST+VwO/X+R68PrwyNkCZDRTtrxTbPfkh+R7BicbiwQT3elQ4/IGiLdJLh3lJyj//ELbT0djcWNsLpbRTpH0DS5o1jjR+vIo/k6WvOF3ov6UNDDFl8Xv2q43ZsLpUhaVOkUg4KQO6G1FM='}}]\n",
      "\n",
      "-------> ENTERING trim_messages:\n",
      "\n",
      "Update the ongoing summary by incorporating the new lines of conversation below.  \n",
      "Build upon the previous summary rather than repeating it so that the result  \n",
      "reflects the most recent context and developments.\n",
      "Previous Summary:\n",
      "\n",
      "\n",
      "New Conversation:\n",
      "ai: What is your question?\n",
      "\n",
      "human: what was he known for/\n",
      "\n",
      "ai: [{'type': 'text', 'text': 'I apologize, but the summary you mentioned was empty, so I don\\'t know who \"he\" is.\\n\\nCould you please provide the name of the person we were discussing so I can tell you what he was known for?', 'extras': {'signature': 'CoMHAXLI2nzezAsLrSj/7XRgQIG6FYHuek3tYWtgjmLzfht8GMIsolSlWcUjPh3fYTnU0mE/XP4vBsRy0Ka+wPpzid8vukbAmn7XdiJE9qw19pA0EqiI3bHS6SW4gD1WY9Y+q/XozGXBZOExU37pJ1IGROJUCDU0Hdc56pGW7n4q9E7S32X9RjLks6Aip3ct3nX3YPJPJaYqOVdzm8Wr7E0l4Jc4WarzE3ymX/uIMRoXHl0fUwEOztj3vLglbqc5XnPMNsqX4ORSXmyTUWo5vh/oDF37Ik7MA1eGnIIL7Ku7HcE+mEU0X05mMS8Ffp1hT3L4LozddDGs7zz3ql6maZjOZvbdvn5d0+0fM0x4qjyNIF6S4Apr+nNqsgeiPRveTC//7+kbMkz9j0YK9GR7iqe/6WpXpfpG+TH9m5+etEjgmo5tGUETB4BGFAv0ybkRvTK7+JNVLlZLMa7gTfL0KIlcdRvNfJHUp9oBCD/kZZJ8Re4e/GSk2BsQyytf5e5j+Em+CukTlygcQy5pvsPKR/GVfY9zcQI6BE3DlIoE2NWtQzP+JHgC2BXiDv9YInYpzAdPdfQ3Cr5+4KcNy5KWWiumdftslwYy7ZqQFpU0+3PaneGEi5qKvmqBdOLZc4xgXy+35UmadDp6VFMkuVbydKgb7rXU+UdMJ3oVtXxFUluUawvfYLJ0hE7XETsq1owOmD3bvO9l6jIHprrNuABByGHhpx94sGoMHlGL8DSjJCg5qNLp42/uLIReR0ooqM7IyEv1k/7VoJ/iA6eLZchKl4qZaIvWC16m4BYs2oS6zUuV8FaSUfbABK/Cdb641sqzhT4lJpJ2VETslhNG4EtV8nViosgruVNCTYRnx1kCVWsgArukZbny0ZQC37JaPY1qlubenDjwcQl0jNjGmoVpgSgt3aBdPeySCSCo3xD5dXMm0YuLA2oZ4HIdhxY6sjKd+OSmpQQkNR/sByD8mysWBUuSIPlLzWCJQxZVCZTN6ezkTfXDsRoRUDKYNwujAGG1OAQCBNhdVojJr8wEpAE76M4SbRrQfLmfGC+q625ST+VwO/X+R68PrwyNkCZDRTtrxTbPfkh+R7BicbiwQT3elQ4/IGiLdJLh3lJyj//ELbT0djcWNsLpbRTpH0DS5o1jjR+vIo/k6WvOF3ov6UNDDFl8Xv2q43ZsLpUhaVOkUg4KQO6G1FM='}}]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [],\n",
       " 'summary': 'The conversation began with the human asking what an unidentified person (\"he\") was known for. The AI was unable to answer this question due to a lack of context (noting that the previous summary was empty) and requested the human to provide the name of the individual.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled.invoke({\"messages\": []},config1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
